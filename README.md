The purpose of this project is educational & self-leearning — to demonstrate how transformer-based large language models (LLMs) like GPT-2 and GPT-3 are built, trained, and used for text generation.

The implementation follows the principles outlined in Sebastian Raschka’s “Building LLMs from Scratch”.
