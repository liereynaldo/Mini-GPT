The purpose of this project is educational & self-learning — to demonstrate how transformer-based large language models (LLMs) like GPT-2 are built, trained, and used for text generation.

The implementation follows the principles outlined in Sebastian Raschka’s “Building LLMs from Scratch”.
